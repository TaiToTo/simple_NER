{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "crazy-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from prepare_invoice_ner_dataset import label_idx_dict\n",
    "from prepare_invoice_ner_dataset import split_tokenize_label_dataset, split_tokenize_label_file, form_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recent-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_path = './split/train.txt'\n",
    "val_split_path = './split/val.txt'\n",
    "test_split_path = './split/test.txt'\n",
    "\n",
    "with open(train_split_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_file_path_list = f.read().splitlines()\n",
    "\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "bert_path = '../kaggle_ner/huggingface-bert/bert-base-uncased/'\n",
    "#model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opening-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'MAX_LEN':128,\n",
    "          'tokenizer': tokenizer,\n",
    "          'batch_size':32,\n",
    "          'Epoch': 3,\n",
    "          #'train_path':train_path,\n",
    "          #'test_path':test_path, \n",
    "          'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "          #'model_path':model_path,\n",
    "          'model_name':'model1_bert_base_uncased_3_epochs.bin'\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "invisible-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_split_and_tokenized_file_list, final_train_split_and_tokenized_labels, final_train_split_word_id_list, final_train_split_token_ids_list = split_tokenize_label_dataset(train_split_path, tokenizer)\n",
    "final_val_split_and_tokenized_file_list, final_val_split_and_tokenized_labels, final_val_split_word_id_list, final_val_split_token_ids_list = split_tokenize_label_dataset(val_split_path, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "discrete-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prod_input = form_input(final_train_split_and_tokenized_file_list, final_train_split_and_tokenized_labels, \n",
    "                              final_train_split_token_ids_list, final_train_split_word_id_list, config, data_type='train')\n",
    "\n",
    "val_prod_input = form_input(final_val_split_and_tokenized_file_list, final_val_split_and_tokenized_labels, \n",
    "                              final_val_split_token_ids_list, final_val_split_word_id_list, config, data_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exclusive-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prod_input_data_loader = DataLoader(train_prod_input, batch_size= config['batch_size'], shuffle=True)\n",
    "val_prod_input_data_loader = DataLoader(val_prod_input, batch_size= config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unusual-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer):\n",
    "    '''\n",
    "    Functiont to train the model\n",
    "    '''\n",
    "    print(\"Training phase\")\n",
    "    train_loss = 0\n",
    "    for index, dataset in enumerate(data_loader):\n",
    "        batch_input_ids = dataset['ids'].to(config['device'], dtype = torch.long)\n",
    "        batch_att_mask = dataset['att_mask'].to(config['device'], dtype = torch.long)\n",
    "        batch_tok_type_id = dataset['tok_type_id'].to(config['device'], dtype = torch.long)\n",
    "        batch_target = dataset['target'].to(config['device'], dtype = torch.long)\n",
    "                \n",
    "        output = model(batch_input_ids, \n",
    "                       token_type_ids=None,\n",
    "                       attention_mask=batch_att_mask,\n",
    "                       labels=batch_target)\n",
    "        \n",
    "        step_loss = output[0]\n",
    "        prediction = output[1]\n",
    "        \n",
    "        if((index+1)%10 == 0):\n",
    "            print(\"Step {}, train loss {}\".format(index+1, step_loss))\n",
    "        \n",
    "        #print(prediction.shape)\n",
    "        \n",
    "        step_loss.sum().backward()\n",
    "        optimizer.step()        \n",
    "        train_loss += step_loss\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    return train_loss.sum()\n",
    "\n",
    "\n",
    "def eval_fn(data_loader, model):\n",
    "    '''\n",
    "    Functiont to evaluate the model on each epoch. \n",
    "    We can also use Jaccard metric to see the performance on each epoch.\n",
    "    '''\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    predictions = np.array([], dtype = np.int64).reshape(0, config['MAX_LEN'])\n",
    "    true_labels = np.array([], dtype = np.int64).reshape(0, config['MAX_LEN'])\n",
    "    print(\"Evaluation phase\")\n",
    "    with torch.no_grad():\n",
    "        for index, dataset in enumerate(data_loader):\n",
    "            batch_input_ids = dataset['ids'].to(config['device'], dtype = torch.long)\n",
    "            batch_att_mask = dataset['att_mask'].to(config['device'], dtype = torch.long)\n",
    "            batch_tok_type_id = dataset['tok_type_id'].to(config['device'], dtype = torch.long)\n",
    "            batch_target = dataset['target'].to(config['device'], dtype = torch.long)\n",
    "\n",
    "            output = model(batch_input_ids, \n",
    "                           token_type_ids=None,\n",
    "                           attention_mask=batch_att_mask,\n",
    "                           labels=batch_target)\n",
    "\n",
    "            step_loss = output[0]\n",
    "            eval_prediction = output[1]\n",
    "\n",
    "            if((index+1)%10 == 0):\n",
    "                print(\"Step {}, train loss {}\".format(index+1, step_loss))\n",
    "            \n",
    "            eval_loss += step_loss\n",
    "            \n",
    "            eval_prediction = np.argmax(eval_prediction.detach().to('cpu').numpy(), axis = 2)\n",
    "            actual = batch_target.to('cpu').numpy()\n",
    "            \n",
    "            predictions = np.concatenate((predictions, eval_prediction), axis = 0)\n",
    "            true_labels = np.concatenate((true_labels, actual), axis = 0)\n",
    "            \n",
    "    return eval_loss.sum(), predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sudden-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_engine(epoch, train_data, valid_data):\n",
    "    model = transformers.BertForTokenClassification.from_pretrained('bert-base-uncased',  num_labels = len(label_idx_dict))\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.to(config['device'])\n",
    "    \n",
    "    params = model.parameters()\n",
    "    optimizer = torch.optim.Adam(params, lr= 3e-5)\n",
    "    \n",
    "    best_eval_loss = 1000000\n",
    "    for i in range(epoch):\n",
    "        train_loss = train_fn(data_loader = train_data, \n",
    "                              model=model, \n",
    "                              optimizer=optimizer)\n",
    "        eval_loss, eval_predictions, true_labels = eval_fn(data_loader = valid_data, \n",
    "                                                           model=model)\n",
    "        \n",
    "        #print(f\"Epoch {i} , Train loss: {train_loss}, Eval loss: {eval_loss}\")\n",
    "\n",
    "        if eval_loss < best_eval_loss:\n",
    "            best_eval_loss = eval_loss           \n",
    "            \n",
    "            print(\"Saving the model\")\n",
    "            torch.save(model.state_dict(), config['model_name'])\n",
    "            \n",
    "    return model, eval_predictions, true_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "robust-homeless",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase\n",
      "Step 10, train loss 0.8367651700973511\n",
      "Step 20, train loss 0.43318334221839905\n",
      "Step 30, train loss 0.3518441617488861\n",
      "Step 40, train loss 0.2847069799900055\n",
      "Step 50, train loss 0.2896549701690674\n",
      "Step 60, train loss 0.19489172101020813\n",
      "Step 70, train loss 0.13789771497249603\n",
      "Step 80, train loss 0.13461090624332428\n",
      "Step 90, train loss 0.04147884249687195\n",
      "Evaluation phase\n",
      "Step 10, train loss 0.05112822353839874\n",
      "Step 20, train loss 0.05208927392959595\n",
      "Step 30, train loss 0.091541588306427\n",
      "Saving the model\n",
      "Training phase\n",
      "Step 10, train loss 0.03879745304584503\n",
      "Step 20, train loss 0.06278297305107117\n",
      "Step 30, train loss 0.05111193656921387\n",
      "Step 40, train loss 0.03180626407265663\n",
      "Step 50, train loss 0.03399399667978287\n",
      "Step 60, train loss 0.019001971930265427\n",
      "Step 70, train loss 0.03240441903471947\n",
      "Step 80, train loss 0.018828539177775383\n",
      "Step 90, train loss 0.015408680774271488\n",
      "Evaluation phase\n",
      "Step 10, train loss 0.06105958670377731\n",
      "Step 20, train loss 0.015644501894712448\n",
      "Step 30, train loss 0.017480891197919846\n",
      "Saving the model\n",
      "Training phase\n",
      "Step 10, train loss 0.009610339067876339\n",
      "Step 20, train loss 0.008739539422094822\n",
      "Step 30, train loss 0.008041378110647202\n",
      "Step 40, train loss 0.012001635506749153\n",
      "Step 50, train loss 0.02154233679175377\n",
      "Step 60, train loss 0.014943626709282398\n",
      "Step 70, train loss 0.004511666484177113\n",
      "Step 80, train loss 0.004468470811843872\n",
      "Step 90, train loss 0.009649300016462803\n",
      "Evaluation phase\n",
      "Step 10, train loss 0.014885790646076202\n",
      "Step 20, train loss 0.03180164843797684\n",
      "Step 30, train loss 0.018785769119858742\n",
      "Saving the model\n"
     ]
    }
   ],
   "source": [
    "model, val_predictions, val_true_labels = train_engine(epoch=config['Epoch'],\n",
    "                                                       train_data=train_prod_input_data_loader, \n",
    "                                                       valid_data=val_prod_input_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-worth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
